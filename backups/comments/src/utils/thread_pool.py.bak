#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ROM Sorter Pro - Fortschrittlicher Thread-Pool
Phase 1 Implementation: Desktop-Optimierung

Dieses Modul bietet eine verbesserte Thread-Pool-Implementierung für die
Parallelverarbeitung von Aufgaben mit fortschrittlicher Fehlerbehandlung,
Ressourcenmanagement und adaptiver Leistungsoptimierung.
"""

import os
import threading
import queue
import time
import logging
import traceback
import concurrent.futures
from typing import Dict, List, Set, Tuple, Optional, Callable, Any, Union

# Logging einrichten
logger = logging.getLogger(__name__)

class Task:
    """Repräsentiert eine Aufgabe, die vom ThreadPool ausgeführt werden soll."""

    def __init__(self, func: Callable, args: tuple = (), kwargs: dict = {},
                task_id: Optional[str] = None, priority: int = 0):
        """
        Initialisiert eine neue Aufgabe.

        Args:
            func: Die auszuführende Funktion
            args: Positionsargumente für die Funktion
            kwargs: Schlüsselwortargumente für die Funktion
            task_id: Optionale ID für die Aufgabe (wird automatisch generiert, wenn nicht angegeben)
            priority: Priorität der Aufgabe (höhere Zahlen = höhere Priorität)
        """
        self.func = func
        self.args = args
        self.kwargs = kwargs
        self.task_id = task_id or f"task_{id(self)}"
        self.priority = priority
        self.creation_time = time.time()
        self.start_time = None
        self.end_time = None
        self.result = None
        self.error = None
        self.status = "pending"  # pending, running, completed, failed, cancelled

    def __lt__(self, other):
        """Vergleich für die Prioritätsqueue - höhere Priorität zuerst."""
        if self.priority != other.priority:
            return self.priority > other.priority  # Higher numbers = higher priority
        return self.creation_time < other.creation_time  # With the same priority: fifo

    def execute(self):
        """Führt die Aufgabe aus und erfasst das Ergebnis oder Fehler."""
        self.start_time = time.time()
        self.status = "running"

        try:
            self.result = self.func(*self.args, **self.kwargs)
            self.status = "completed"
        except Exception as e:
            self.error = e
            self.status = "failed"
            logger.error(f"Task {self.task_id} failed: {str(e)}")
            logger.debug(traceback.format_exc())

        self.end_time = time.time()
        return self

class AdaptiveThreadPool:
    """
    Ein Thread-Pool mit adaptiver Leistungsoptimierung, Prioritätsunterstützung
    und erweiterten Überwachungsfunktionen.
    """

    def __init__(self, min_workers: int = 2, max_workers: int = None,
                name_prefix: str = "worker", daemon: bool = True):
        """
        Initialisiert den Thread-Pool.

        Args:
            min_workers: Minimale Anzahl von Worker-Threads
            max_workers: Maximale Anzahl von Worker-Threads (None für CPU-Anzahl * 2)
            name_prefix: Präfix für Thread-Namen
            daemon: Ob die Threads als Daemon-Threads laufen sollen
        """
        self.min_workers = min_workers
        self.max_workers = max_workers or (os.cpu_count() or 4) * 2
        self.name_prefix = name_prefix
        self.daemon = daemon

        # Task Queues and Worker Management
        self.task_queue = queue.PriorityQueue()
        self.workers = []
        self.active_workers = 0
        self.total_tasks_submitted = 0
        self.total_tasks_completed = 0
        self.total_tasks_failed = 0

        # Callbacks
        self.on_task_start = None  # Callback: (task: Task) -> None
        self.on_task_complete = None  # Callback: (task: Task) -> None
        self.on_task_error = None  # Callback: (task: Task, error: Exception) -> None

        # Status and synchronization
        self.running = False
        self._lock = threading.RLock()
        self._worker_event = threading.Event()
        self._shutdown_event = threading.Event()

        # Performance monitoring and adaptation
        self.performance_metrics = {
            'avg_task_time': 0.0,
            'avg_queue_time': 0.0,
            'throughput': 0.0,
            'last_adjustment_time': 0.0
        }
        self.adjustment_interval = 10.0  # Sekunden zwischen Anpassungen

        # Thread for performance monitoring and worker adjustment
        self.monitor_thread = None

    def start(self):
        """Startet den Thread-Pool."""
        with self._lock:
            if self.running:
                return

            self.running = True
            self._shutdown_event.clear()

            # Initialize the worker threads
            for _ in range(self.min_workers):
                self._add_worker()

            # Start the surveillance thread
            self.monitor_thread = threading.Thread(
                target=self._monitor_performance,
                name=f"{self.name_prefix}_monitor",
                daemon=True
            )
            self.monitor_thread.start()

            logger.info(f"AdaptiveThreadPool gestartet mit {self.min_workers} Workern")

    def shutdown(self, wait: bool = True):
        """
        Fährt den Thread-Pool herunter.

        Args:
            wait: Ob auf die Beendigung aller Tasks gewartet werden soll
        """
        with self._lock:
            if not self.running:
                return

            self.running = False
            self._shutdown_event.set()
            self._worker_event.set()  # Weckt alle wartenden Worker auf

        if wait:
            # Warte auf Beendigung aller Worker
            for worker in self.workers:
                if worker.is_alive():
                    worker.join()

        logger.info("AdaptiveThreadPool heruntergefahren")

    def submit(self, func: Callable, *args, **kwargs) -> str:
        """
        Sendet eine Aufgabe zur Ausführung.

        Args:
            func: Die auszuführende Funktion
            *args: Positionsargumente für die Funktion
            **kwargs: Schlüsselwortargumente für die Funktion

            Zusätzliche Schlüsselwortargumente:
            - task_id: Optionale ID für die Aufgabe
            - priority: Priorität (höhere Zahlen = höhere Priorität)

        Returns:
            Die ID der Aufgabe
        """
        # Spezielle Kwargs extrahieren
        task_id = kwargs.pop('task_id', None)
        priority = kwargs.pop('priority', 0)

        task = Task(func, args, kwargs, task_id, priority)

        with self._lock:
            self.total_tasks_submitted += 1
            self.task_queue.put(task)
            self._worker_event.set()  # Weckt einen wartenden Worker auf

        # Check whether we need more worker
        self._check_worker_count()

        return task.task_id

    def _add_worker(self):
        """Fügt einen neuen Worker-Thread zum Pool hinzu."""
        with self._lock:
            if len(self.workers) >= self.max_workers:
                return False

            worker_id = len(self.workers) + 1
            worker = threading.Thread(
                target=self._worker_loop,
                name=f"{self.name_prefix}_{worker_id}",
                daemon=self.daemon
            )
            self.workers.append(worker)
            worker.start()

            logger.debug(f"Worker {worker.name} gestartet")
            return True

    def _worker_loop(self):
        """Hauptschleife für Worker-Threads."""
        while self.running and not self._shutdown_event.is_set():
            try:
                # Try to get a task
                try:
                    # Wait for a task for a short time
                    task = self.task_queue.get(timeout=0.1)
                except queue.Empty:
                    # No task available, check whether we should end
                    if self._shutdown_event.is_set():
                        break

                    # Wait for a signal or timeout
                    self._worker_event.wait(1.0)
                    self._worker_event.clear()
                    continue

                # Found task, lead them out
                with self._lock:
                    self.active_workers += 1

                if self.on_task_start:
                    try:
                        self.on_task_start(task)
                    except Exception as e:
                        logger.error(f"Fehler im on_task_start-Callback: {e}")

                # Perform the task
                task.execute()

                # Aktualisiere Statistiken
                with self._lock:
                    self.active_workers -= 1

                    if task.status == "completed":
                        self.total_tasks_completed += 1

                        # Aktualisiere Performance-Metriken
                        task_time = task.end_time - task.start_time
                        queue_time = task.start_time - task.creation_time

                        # Exponentieller gleitender Durchschnitt
                        alpha = 0.1  # Gewichtungsfaktor
                        self.performance_metrics['avg_task_time'] = (
                            (1 - alpha) * self.performance_metrics['avg_task_time'] +
                            alpha * task_time
                        )
                        self.performance_metrics['avg_queue_time'] = (
                            (1 - alpha) * self.performance_metrics['avg_queue_time'] +
                            alpha * queue_time
                        )

                        if self.on_task_complete:
                            try:
                                self.on_task_complete(task)
                            except Exception as e:
                                logger.error(f"Fehler im on_task_complete-Callback: {e}")

                    elif task.status == "failed":
                        self.total_tasks_failed += 1

                        if self.on_task_error:
                            try:
                                self.on_task_error(task, task.error)
                            except Exception as e:
                                logger.error(f"Fehler im on_task_error-Callback: {e}")

                # Mark the task as done
                self.task_queue.task_done()

            except Exception as e:
                logger.error(f"Unerwarteter Fehler im Worker-Thread: {e}")
                logger.debug(traceback.format_exc())

    def _check_worker_count(self):
        """Überprüft, ob die Anzahl der Worker angepasst werden muss."""
        now = time.time()

        # Do not adapt too often
        if now - self.performance_metrics['last_adjustment_time'] < self.adjustment_interval:
            return

        with self._lock:
            queue_size = self.task_queue.qsize()
            current_workers = len(self.workers)
            active_workers = self.active_workers

            # When the cue grows and we have active workers
            # Let's add more workers
            if queue_size > current_workers * 2 and active_workers >= current_workers * 0.8:
                workers_to_add = min(
                    queue_size // 4,  # A maximum of 1/4 of the queue size
                    self.max_workers - current_workers  # But no more than maximum allowed
                )

                for _ in range(workers_to_add):
                    if not self._add_worker():
                        break

                if workers_to_add > 0:
                    logger.info(f"{workers_to_add} Worker hinzugefügt (jetzt {len(self.workers)})")

            # Update time of the last adjustment
            self.performance_metrics['last_adjustment_time'] = now

    def _monitor_performance(self):
        """Überwacht die Performance und passt die Anzahl der Worker an."""
        while self.running and not self._shutdown_event.is_set():
            try:
                # Sleep for an interval
                time.sleep(self.adjustment_interval)

                with self._lock:
                    if not self.running or self._shutdown_event.is_set():
                        break

                    # Berechne Durchsatz (Tasks pro Sekunde)
                    now = time.time()
                    elapsed = now - self.performance_metrics['last_adjustment_time']
                    if elapsed > 0:
                        current_throughput = (
                            self.total_tasks_completed / elapsed
                            if self.total_tasks_completed > 0 else 0
                        )

                        # Aktualisiere den durchschnittlichen Durchsatz
                        alpha = 0.2  # Gewichtungsfaktor
                        self.performance_metrics['throughput'] = (
                            (1 - alpha) * self.performance_metrics['throughput'] +
                            alpha * current_throughput
                        )

                    # Check whether we have to adjust the number of workers
                    self._check_worker_count()

                    # Update the time of the last adaptation
                    self.performance_metrics['last_adjustment_time'] = now

                    # Log Performance-Metriken
                    logger.debug(
                        f"Performance: Worker={len(self.workers)}, "
                        f"Aktiv={self.active_workers}, "
                        f"Queue={self.task_queue.qsize()}, "
                        f"Avg Task Time={self.performance_metrics['avg_task_time']:.3f}s, "
                        f"Throughput={self.performance_metrics['throughput']:.2f} tasks/s"
                    )

            except Exception as e:
                logger.error(f"Fehler im Performance-Monitoring-Thread: {e}")
                logger.debug(traceback.format_exc())

    def wait_completion(self):
        """Wartet, bis alle Aufgaben abgeschlossen sind."""
        self.task_queue.join()

    @property
    def stats(self) -> Dict[str, Any]:
        """Gibt aktuelle Statistiken des Thread-Pools zurück."""
        with self._lock:
            return {
                'workers': len(self.workers),
                'active_workers': self.active_workers,
                'queue_size': self.task_queue.qsize(),
                'submitted': self.total_tasks_submitted,
                'completed': self.total_tasks_completed,
                'failed': self.total_tasks_failed,
                'avg_task_time': self.performance_metrics['avg_task_time'],
                'avg_queue_time': self.performance_metrics['avg_queue_time'],
                'throughput': self.performance_metrics['throughput']
            }

class BatchProcessor:
    """
    Verarbeitet Batches von Aufgaben mit einem AdaptiveThreadPool
    und bietet verbesserte Funktionen für Batch-Verarbeitung.
    """

    def __init__(self, min_workers: int = 2, max_workers: int = None):
        """
        Initialisiert den Batch-Prozessor.

        Args:
            min_workers: Minimale Anzahl von Worker-Threads
            max_workers: Maximale Anzahl von Worker-Threads
        """
        self.thread_pool = AdaptiveThreadPool(
            min_workers=min_workers,
            max_workers=max_workers,
            name_prefix="batch_worker"
        )

        # Batch-Status
        self.batches = {}
        self.batch_counter = 0
        self.current_batch = None

        # Callbacks
        self.on_batch_start = None
        self.on_batch_complete = None
        self.on_batch_progress = None

        # Status and synchronization
        self._lock = threading.RLock()

    def start(self):
        """Startet den Thread-Pool."""
        self.thread_pool.start()

        # Callbacks verbinden
        self.thread_pool.on_task_complete = self._on_task_complete
        self.thread_pool.on_task_error = self._on_task_error

    def shutdown(self, wait: bool = True):
        """
        Fährt den Thread-Pool herunter.

        Args:
            wait: Ob auf die Beendigung aller Tasks gewartet werden soll
        """
        self.thread_pool.shutdown(wait)

    def create_batch(self, name: str = None) -> str:
        """
        Erstellt einen neuen Batch und gibt dessen ID zurück.

        Args:
            name: Optionaler Name für den Batch

        Returns:
            Die Batch-ID
        """
        with self._lock:
            self.batch_counter += 1
            batch_id = f"batch_{self.batch_counter}"

            self.batches[batch_id] = {
                'name': name or batch_id,
                'status': 'created',
                'tasks': {},
                'creation_time': time.time(),
                'start_time': None,
                'end_time': None,
                'total_tasks': 0,
                'completed_tasks': 0,
                'failed_tasks': 0
            }

            return batch_id

    def start_batch(self, batch_id: str) -> bool:
        """
        Startet die Ausführung eines Batches.

        Args:
            batch_id: Die Batch-ID

        Returns:
            True wenn der Batch erfolgreich gestartet wurde, False sonst
        """
        with self._lock:
            if batch_id not in self.batches:
                logger.error(f"Batch {batch_id} existiert nicht")
                return False

            batch = self.batches[batch_id]
            if batch['status'] != 'created':
                logger.error(f"Batch {batch_id} hat bereits den Status {batch['status']}")
                return False

            batch['status'] = 'running'
            batch['start_time'] = time.time()

            # Callback aufrufen
            if self.on_batch_start:
                try:
                    self.on_batch_start(batch_id, batch)
                except Exception as e:
                    logger.error(f"Fehler im on_batch_start-Callback: {e}")

            return True

    def add_task(self, batch_id: str, func: Callable, *args, **kwargs) -> Optional[str]:
        """
        Fügt eine Aufgabe zu einem Batch hinzu.

        Args:
            batch_id: Die Batch-ID
            func: Die auszuführende Funktion
            *args: Positionsargumente für die Funktion
            **kwargs: Schlüsselwortargumente für die Funktion

        Returns:
            Die Task-ID oder None, wenn der Batch nicht existiert
        """
        with self._lock:
            if batch_id not in self.batches:
                logger.error(f"Batch {batch_id} existiert nicht")
                return None

            batch = self.batches[batch_id]
            if batch['status'] not in ['created', 'running']:
                logger.error(f"Batch {batch_id} hat den Status {batch['status']}")
                return None

            # Create task ID for tracking
            task_kwargs = kwargs.copy()
            task_id = task_kwargs.pop('task_id', None) or f"{batch_id}_task_{batch['total_tasks'] + 1}"

            # Add task to the thread pool
            task_id = self.thread_pool.submit(
                func, *args, **task_kwargs, task_id=task_id, batch_id=batch_id
            )

            # Task in Batch registrieren
            batch['tasks'][task_id] = {
                'status': 'submitted',
                'creation_time': time.time()
            }
            batch['total_tasks'] += 1

            return task_id

    def add_tasks_bulk(self, batch_id: str, tasks: List[Tuple[Callable, tuple, dict]]) -> List[Optional[str]]:
        """
        Fügt mehrere Aufgaben auf einmal zu einem Batch hinzu.

        Args:
            batch_id: Die Batch-ID
            tasks: Liste von (func, args, kwargs) Tupeln

        Returns:
            Liste von Task-IDs oder None-Werten
        """
        task_ids = []

        for func, args, kwargs in tasks:
            task_id = self.add_task(batch_id, func, *args, **kwargs)
            task_ids.append(task_id)

        return task_ids

    def _on_task_complete(self, task: Task):
        """
        Wird aufgerufen, wenn eine Aufgabe abgeschlossen ist.

        Args:
            task: Die abgeschlossene Aufgabe
        """
        batch_id = task.kwargs.get('batch_id')
        if not batch_id:
            return

        with self._lock:
            if batch_id not in self.batches:
                return

            batch = self.batches[batch_id]
            if task.task_id in batch['tasks']:
                batch['tasks'][task.task_id]['status'] = 'completed'
                batch['tasks'][task.task_id]['end_time'] = time.time()
                batch['completed_tasks'] += 1

                # Check whether the batch is complete
                if batch['completed_tasks'] + batch['failed_tasks'] >= batch['total_tasks']:
                    self._finish_batch(batch_id)
                else:
                    # Progress-Callback aufrufen
                    if self.on_batch_progress:
                        try:
                            progress = batch['completed_tasks'] / batch['total_tasks']
                            self.on_batch_progress(batch_id, batch, progress)
                        except Exception as e:
                            logger.error(f"Fehler im on_batch_progress-Callback: {e}")

    def _on_task_error(self, task: Task, error: Exception):
        """
        Wird aufgerufen, wenn eine Aufgabe fehlschlägt.

        Args:
            task: Die fehlgeschlagene Aufgabe
            error: Der aufgetretene Fehler
        """
        batch_id = task.kwargs.get('batch_id')
        if not batch_id:
            return

        with self._lock:
            if batch_id not in self.batches:
                return

            batch = self.batches[batch_id]
            if task.task_id in batch['tasks']:
                batch['tasks'][task.task_id]['status'] = 'failed'
                batch['tasks'][task.task_id]['end_time'] = time.time()
                batch['tasks'][task.task_id]['error'] = str(error)
                batch['failed_tasks'] += 1

                # Check whether the batch is complete
                if batch['completed_tasks'] + batch['failed_tasks'] >= batch['total_tasks']:
                    self._finish_batch(batch_id)
                else:
                    # Progress-Callback aufrufen
                    if self.on_batch_progress:
                        try:
                            progress = batch['completed_tasks'] / batch['total_tasks']
                            self.on_batch_progress(batch_id, batch, progress)
                        except Exception as e:
                            logger.error(f"Fehler im on_batch_progress-Callback: {e}")

    def _finish_batch(self, batch_id: str):
        """
        Schließt einen Batch ab.

        Args:
            batch_id: Die Batch-ID
        """
        with self._lock:
            if batch_id not in self.batches:
                return

            batch = self.batches[batch_id]
            batch['status'] = 'completed'
            batch['end_time'] = time.time()

            # Callback aufrufen
            if self.on_batch_complete:
                try:
                    self.on_batch_complete(batch_id, batch)
                except Exception as e:
                    logger.error(f"Fehler im on_batch_complete-Callback: {e}")

    def get_batch_status(self, batch_id: str) -> Optional[Dict]:
        """
        Gibt den Status eines Batches zurück.

        Args:
            batch_id: Die Batch-ID

        Returns:
            Ein Dictionary mit Status-Informationen oder None, wenn der Batch nicht existiert
        """
        with self._lock:
            if batch_id not in self.batches:
                return None

            batch = self.batches[batch_id].copy()

            # Calculate additional information
            if batch['status'] == 'running' and batch['total_tasks'] > 0:
                batch['progress'] = batch['completed_tasks'] / batch['total_tasks']

                if batch['start_time']:
                    elapsed = time.time() - batch['start_time']
                    if batch['completed_tasks'] > 0 and elapsed > 0:
                        # Estimate the remaining time
                        items_per_second = batch['completed_tasks'] / elapsed
                        remaining_items = batch['total_tasks'] - batch['completed_tasks']
                        estimated_seconds = remaining_items / items_per_second if items_per_second > 0 else 0

                        batch['elapsed_seconds'] = elapsed
                        batch['estimated_seconds_remaining'] = estimated_seconds

            return batch

    def get_all_batches(self) -> Dict[str, Dict]:
        """
        Gibt den Status aller Batches zurück.

        Returns:
            Ein Dictionary mit Batch-IDs als Schlüssel und Status-Dictionaries als Werte
        """
        with self._lock:
            return {batch_id: self.get_batch_status(batch_id) for batch_id in self.batches}

    def wait_for_batch(self, batch_id: str, timeout: Optional[float] = None) -> bool:
        """
        Wartet, bis ein Batch abgeschlossen ist.

        Args:
            batch_id: Die Batch-ID
            timeout: Optionales Timeout in Sekunden

        Returns:
            True wenn der Batch abgeschlossen wurde, False bei Timeout
        """
        start_time = time.time()

        while True:
            status = self.get_batch_status(batch_id)

            if not status:
                logger.error(f"Batch {batch_id} existiert nicht")
                return False

            if status['status'] == 'completed':
                return True

            # Check on timeout
            if timeout and time.time() - start_time > timeout:
                return False

            # Short break to protect CPU
            time.sleep(0.1)

# Main function for test purposes
def main():
    """Testfunktion für den AdaptiveThreadPool und BatchProcessor."""
    # Logging konfigurieren
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Beispielaufgabe
    def example_task(task_id, sleep_time):
        logger.info(f"Task {task_id} gestartet (Schlafzeit: {sleep_time}s)")
        time.sleep(sleep_time)
        logger.info(f"Task {task_id} abgeschlossen")
        return f"Ergebnis von Task {task_id}"

    # Test for adaptive thread pool
    def test_thread_pool():
        logger.info("Teste AdaptiveThreadPool...")

        pool = AdaptiveThreadPool(min_workers=2, max_workers=8)
        pool.start()

        # Add tasks
        for i in range(20):
            sleep_time = 0.5 + (i % 5) * 0.3  # Vary between 0.5 and 1.7 seconds
            pool.submit(example_task, f"pool_{i}", sleep_time)

        # Warte auf Abschluss
        pool.wait_completion()
        logger.info(f"ThreadPool-Statistiken: {pool.stats}")

        # Pool herunterfahren
        pool.shutdown()
        logger.info("AdaptiveThreadPool-Test abgeschlossen")

    # Test for batchprocessor
    def test_batch_processor():
        logger.info("Teste BatchProcessor...")

        processor = BatchProcessor(min_workers=2, max_workers=8)
        processor.start()

        # Callback for batch progress
        def on_progress(batch_id, batch, progress):
            logger.info(f"Batch {batch_id}: {progress*100:.1f}% abgeschlossen")

        processor.on_batch_progress = on_progress

        # Erstelle einen Batch
        batch_id = processor.create_batch("Testbatch")
        processor.start_batch(batch_id)

        # Add tasks
        for i in range(10):
            sleep_time = 0.2 + (i % 3) * 0.3  # Varized between 0.2 and 0.8 seconds
            processor.add_task(batch_id, example_task, f"batch_{i}", sleep_time)

        # Warte auf Abschluss des Batches
        processor.wait_for_batch(batch_id)
        logger.info(f"Batch-Status: {processor.get_batch_status(batch_id)}")

        # Processor herunterfahren
        processor.shutdown()
        logger.info("BatchProcessor-Test abgeschlossen")

    # Perform tests
    test_thread_pool()
    print("\n" + "-" * 60 + "\n")
    test_batch_processor()

if __name__ == "__main__":
    main()
